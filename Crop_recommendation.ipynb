{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "wiGWkPRLIE5f",
    "outputId": "91f3d562-088c-4619-d932-65b9031d7750"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P47KJM2GYe3b",
    "outputId": "30ee60c4-c142-4e04-e33a-49678a56cd51"
   },
   "outputs": [],
   "source": [
    "# üöÄ Install all required libraries correctly\n",
    "!pip install -U pandas scikit-learn imbalanced-learn xgboost catboost tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5OHJrvPTWyt",
    "outputId": "b4df75f3-51a7-41d7-9c17-eda786cc9734"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Set the filename to your dataset\n",
    "DATASET_FILENAME = \"Crop Recommendation using Soil Properties and Weather Prediction.csv\"\n",
    "SAMPLE_ROWS = 10\n",
    "\n",
    "print(f\"üîπ Loading dataset and checking feature names: {DATASET_FILENAME}...\")\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(DATASET_FILENAME)\n",
    "    print(\"‚úÖ Dataset loaded successfully.\")\n",
    "\n",
    "    # --- Actual Columns Found (Printed for verification) ---\n",
    "    actual_columns = df.columns.tolist()\n",
    "    print(\"\\n‚ö†Ô∏è **Actual Columns Found in Dataset:**\")\n",
    "    print(actual_columns)\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "    # --- CORRECTED FEATURE LIST (Using EXACT names from your output) ---\n",
    "    # We will select the core nutrients, soil pH, soil color,\n",
    "    # and a representative set of the complex weather features (e.g., Winter values).\n",
    "    NECESSARY_FEATURES = [\n",
    "        'N', 'P', 'K',             # Major Nutrients\n",
    "        'Ph',                      # Corrected: Ph (Capital P)\n",
    "        'Soilcolor',               # Corrected: Soilcolor (One word, Capital S)\n",
    "        'PRECTOTCORR-W',           # Corrected: Placeholder for Winter Precipitation/Rainfall\n",
    "        'T2M_MAX-W',               # Corrected: Placeholder for Winter Max Temperature\n",
    "        'QV2M-W',                  # Corrected: Placeholder for Winter Humidity/Moisture content\n",
    "        'label'                    # The target variable\n",
    "    ]\n",
    "\n",
    "    # Ensure all required features are present before proceeding\n",
    "    for feature in NECESSARY_FEATURES:\n",
    "        if feature not in actual_columns:\n",
    "            raise KeyError(f\"The necessary feature '{feature}' was not found in the dataset columns.\")\n",
    "\n",
    "    # 1. Filter the DataFrame to include only the necessary columns\n",
    "    df_selected = df[NECESSARY_FEATURES]\n",
    "\n",
    "    # 2. Print Sample Dataset (Head) of the selected columns\n",
    "    print(f\"\\nüìã Sample Dataset (First {SAMPLE_ROWS} rows) - Selected Features:\")\n",
    "\n",
    "    print(tabulate(df_selected.head(SAMPLE_ROWS),\n",
    "                   headers='keys',\n",
    "                   tablefmt='fancy_grid',\n",
    "                   showindex=False))\n",
    "\n",
    "    print(f\"\\n‚úÖ Columns successfully displayed: {NECESSARY_FEATURES}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"\\n‚ùå ERROR: Feature mismatch! {e}\")\n",
    "    print(\"ACTION: You must ensure the `NECESSARY_FEATURES` list EXACTLY matches the names printed above.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå ERROR: The file '{DATASET_FILENAME}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKNDcHsTUIAl",
    "outputId": "8deac728-5bbb-48ec-9b67-c81be46618ea"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the filename to your dataset\n",
    "DATASET_FILENAME = \"Crop Recommendation using Soil Properties and Weather Prediction.csv\"\n",
    "\n",
    "print(f\"üîπ Loading dataset: {DATASET_FILENAME}...\")\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(DATASET_FILENAME)\n",
    "    print(\"‚úÖ Dataset loaded successfully.\")\n",
    "\n",
    "    # Get the list of all column names (features + target)\n",
    "    all_features_list = df.columns.tolist()\n",
    "\n",
    "    print(\"\\nüìã **All Features (Column Names) Present in the Dataset:**\")\n",
    "    # Print them clearly as a list\n",
    "    for i, col in enumerate(all_features_list):\n",
    "        print(f\"{i+1}. {col}\")\n",
    "\n",
    "    # You can also print the list directly\n",
    "    # print(\"\\nComplete List:\")\n",
    "    # print(all_features_list)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå ERROR: The file '{DATASET_FILENAME}' was not found. Please ensure it is uploaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgXXlFs2I6yx",
    "outputId": "cf23dfb0-6897-4c0b-c2fb-8d0464e09103"
   },
   "outputs": [],
   "source": [
    "print(\"üîπ Installing required libraries...\")\n",
    "# We add 'tabulate' for printing clean tables\n",
    "!pip install pandas scikit-learn \"imblearn>=0.11.0\" joblib matplotlib seaborn \"tensorflow>=2.12\" \"xgboost>=1.7\" \"catboost>=1.2\" tabulate\n",
    "print(\"‚úÖ Installation complete!\")\n",
    "\n",
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier,\n",
    "    BaggingClassifier\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Import standard regularizers AND the base class to build our custom L3\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2, Regularizer\n",
    "from tabulate import tabulate # For clean table output\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOHOIqaXZrJ3",
    "outputId": "d01af89b-22b9-4a65-cbc0-e034fe21031c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load dataset\n",
    "print(\"üîπ Loading raw dataset...\")\n",
    "# Make sure \"Crop Recommendation using Soil Properties and Weather Prediction.csv\" is uploaded to Colab\n",
    "df = pd.read_csv(\"Crop Recommendation using Soil Properties and Weather Prediction.csv\")\n",
    "print(f\"Dataset shape before preprocessing: {df.shape}\")\n",
    "\n",
    "# Identify categorical (non-numeric) columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"\\nüîπ Categorical columns found:\", categorical_cols)\n",
    "\n",
    "# Encode label column separately\n",
    "target_col = 'label'\n",
    "categorical_cols.remove(target_col) # Remove 'label' from features to be encoded\n",
    "\n",
    "# Encode other categorical features (e.g., soil color)\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Encoded column: {col}\")\n",
    "\n",
    "# Encode the target (crop name)\n",
    "le_target = LabelEncoder()\n",
    "df['label_encoded'] = le_target.fit_transform(df[target_col])\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop([target_col, 'label_encoded'], axis=1)\n",
    "y = df['label_encoded']\n",
    "print(f\"\\nüîπ Features (X) shape: {X.shape}\")\n",
    "\n",
    "# Scale numeric features\n",
    "print(\"\\nüîπ Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply SMOTE for balancing\n",
    "print(\"\\nüîπ Applying SMOTE for class balancing...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "print(\"‚úÖ Balancing complete!\")\n",
    "print(\"Samples after SMOTE:\", len(y_resampled))\n",
    "\n",
    "# Create new DataFrame\n",
    "df_balanced = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "df_balanced['label_encoded'] = y_resampled\n",
    "\n",
    "# Save processed data\n",
    "df_balanced.to_csv(\"balanced_dataset.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Preprocessing complete! Processed dataset saved as 'balanced_dataset.csv' with shape {df_balanced.shape}\")\n",
    "\n",
    "# Save encoders and scaler\n",
    "joblib.dump(le_target, \"label_encoder.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(label_encoders, \"feature_encoders.pkl\")\n",
    "print(\"‚úÖ Encoders and scaler saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82HHlmP6Jhqo",
    "outputId": "8db41997-1399-4fcd-a919-11acc7e2b453"
   },
   "outputs": [],
   "source": [
    "print(\"üîπ Loading preprocessed balanced dataset...\")\n",
    "data = pd.read_csv(\"balanced_dataset.csv\")\n",
    "print(f\"‚úÖ Balanced dataset shape: {data.shape}\")\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop('label_encoded', axis=1)\n",
    "y = data['label_encoded']\n",
    "\n",
    "# Get number of classes for later\n",
    "num_classes = len(np.unique(y))\n",
    "print(f\"Number of unique classes: {num_classes}\")\n",
    "\n",
    "# Split into Train/Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"‚úÖ Data split complete ‚Äî 80% train, 20% test\")\n",
    "print(f\"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grqDT3nbJkxX",
    "outputId": "20517de5-2827-4815-b57e-2a5119ef83c4"
   },
   "outputs": [],
   "source": [
    "print(\"üöÄ Training standard ML models... please wait...\")\n",
    "\n",
    "# Define Models\n",
    "rf_clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "ada_clf = AdaBoostClassifier(n_estimators=150, random_state=42)\n",
    "cat_clf = CatBoostClassifier(verbose=0, allow_writing_files=False, random_state=42)\n",
    "\n",
    "# Train Models\n",
    "rf_clf.fit(X_train, y_train)\n",
    "print(\"‚úÖ Random Forest trained.\")\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "print(\"‚úÖ XGBoost trained.\")\n",
    "ada_clf.fit(X_train, y_train)\n",
    "print(\"‚úÖ AdaBoost trained.\")\n",
    "cat_clf.fit(X_train, y_train)\n",
    "print(\"‚úÖ CatBoost trained.\")\n",
    "\n",
    "print(\"\\n‚úÖ All base models trained successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTCSXnD8Jo3t",
    "outputId": "c368b482-7b13-40db-d737-131cb842301d"
   },
   "outputs": [],
   "source": [
    "print(\"üìä Evaluating Individual Models...\")\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": rf_clf,\n",
    "    \"XGBoost\": xgb_clf,\n",
    "    \"AdaBoost\": ada_clf,\n",
    "    \"CatBoost\": cat_clf\n",
    "}\n",
    "\n",
    "# This dictionary will hold all model accuracies for the final chart\n",
    "model_accuracies = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    model_accuracies[name] = acc\n",
    "    print(f\"üî∏ {name} Accuracy: {acc:.4f}\")\n",
    "    # print(classification_report(y_test, y_pred)) # Uncomment to see full report\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QxSN90TJJtit",
    "outputId": "e617131b-889c-4854-ac8b-bda525df6c38"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüß© Training and Evaluating Ensemble Models...\")\n",
    "\n",
    "# Voting Ensemble (Hard Voting)\n",
    "print(\"üîπ Training Voting Ensemble...\")\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('xgb', xgb_clf), ('ada', ada_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_pred = voting_clf.predict(X_test)\n",
    "voting_acc = accuracy_score(y_test, voting_pred)\n",
    "model_accuracies[\"Voting Ensemble\"] = voting_acc\n",
    "print(f\"‚úÖ Voting Ensemble Accuracy: {voting_acc:.4f}\")\n",
    "\n",
    "# Stacking Ensemble\n",
    "print(\"\\nüîπ Training Stacking Ensemble...\")\n",
    "estimators = [\n",
    "    ('rf', rf_clf),\n",
    "    ('xgb', xgb_clf),\n",
    "    ('cat', cat_clf)\n",
    "]\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    ")\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "stack_pred = stacking_clf.predict(X_test)\n",
    "stack_acc = accuracy_score(y_test, stack_pred)\n",
    "model_accuracies[\"Stacking Ensemble\"] = stack_acc\n",
    "print(f\"‚úÖ Stacking Ensemble Accuracy: {stack_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwz4zDjwJwFS",
    "outputId": "340086c2-d6f3-42c8-f1d5-ab0a06c55eb8"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüß† NEW: Starting CNN Optimizer/Activation Experiment...\")\n",
    "\n",
    "# Prepare data for 1D-CNN\n",
    "X_train_cnn = np.expand_dims(X_train.values, axis=2)\n",
    "X_test_cnn = np.expand_dims(X_test.values, axis=2)\n",
    "y_train_cnn = to_categorical(y_train, num_classes)\n",
    "y_test_cnn = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"CNN Train data shape: {X_train_cnn.shape}, CNN Test data shape: {X_test_cnn.shape}\")\n",
    "\n",
    "# Define experiment parameters\n",
    "optimizers_list = ['adam', 'rmsprop']\n",
    "activations_list = ['relu', 'gelu', 'tanh', 'sigmoid']\n",
    "epochs_to_run = 50\n",
    "input_shape = (X_train_cnn.shape[1], 1)\n",
    "\n",
    "# This list will store results for the table\n",
    "cnn_results_list = []\n",
    "best_accuracy = 0\n",
    "best_combo = {} # To store the best optimizer/activation for the next cell\n",
    "\n",
    "# Helper function to create the model\n",
    "def create_cnn_model(optimizer_name, activation_name, regularizer=None):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv1D(64, 3, activation=activation_name),\n",
    "        Conv1D(32, 3, activation=activation_name),\n",
    "        Flatten(),\n",
    "        Dropout(0.3),\n",
    "        # Apply regularizer to the dense layer\n",
    "        Dense(64, activation=activation_name, kernel_regularizer=regularizer),\n",
    "        Dense(num_classes, activation='softmax') # Output layer\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer_name,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Run the experiment loop\n",
    "for optimizer_name in optimizers_list:\n",
    "    for activation_name in activations_list:\n",
    "        combo_name = f\"CNN ({optimizer_name} + {activation_name})\"\n",
    "        print(f\"\\nüöÄ Training {combo_name} for {epochs_to_run} epochs...\")\n",
    "\n",
    "        model = create_cnn_model(optimizer_name, activation_name)\n",
    "\n",
    "        model.fit(X_train_cnn, y_train_cnn,\n",
    "                  epochs=epochs_to_run,\n",
    "                  batch_size=32,\n",
    "                  verbose=0, # Set to 1 to see epoch-by-epoch progress\n",
    "                  validation_split=0.1)\n",
    "\n",
    "        loss, accuracy = model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
    "\n",
    "        print(f\"‚úÖ {combo_name} - Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Store result for table and main dictionary\n",
    "        result_entry = {\n",
    "            \"Optimizer\": optimizer_name,\n",
    "            \"Activation\": activation_name,\n",
    "            \"Accuracy\": accuracy\n",
    "        }\n",
    "        cnn_results_list.append(result_entry)\n",
    "        model_accuracies[combo_name] = accuracy\n",
    "\n",
    "        # Track the best combination\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_combo = {\"optimizer\": optimizer_name, \"activation\": activation_name}\n",
    "\n",
    "print(\"\\n\\n‚úÖ CNN Experiment Complete!\")\n",
    "\n",
    "# --- Generate and Print Results Table ---\n",
    "results_df = pd.DataFrame(cnn_results_list)\n",
    "results_df = results_df.sort_values(by=\"Accuracy\", ascending=False)\n",
    "results_df[\"Accuracy\"] = results_df[\"Accuracy\"].map(lambda x: f\"{x:.4f}\") # Format for printing\n",
    "\n",
    "print(\"\\nüìä CNN Optimizer/Activation Accuracy Comparison\")\n",
    "print(tabulate(results_df, headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "print(f\"\\nüèÜ Best Combination Found: {best_combo['optimizer']} + {best_combo['activation']} with {best_accuracy:.4f} accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8C8dnfgJysa",
    "outputId": "57ff0bf0-4cec-4388-d45b-5ebdf579d67d"
   },
   "outputs": [],
   "source": [
    "# --- Define Custom L3 Regularizer ---\n",
    "# We build a class that Keras can use\n",
    "# It calculates: strength * sum(abs(weights)^3)\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable() # Helps save/load models with this\n",
    "class L3Regularizer(Regularizer):\n",
    "    \"\"\"\n",
    "    A custom L3 regularizer that applies a penalty based on the L3 norm.\n",
    "    Penalty = strength * sum(abs(w)^3)\n",
    "    \"\"\"\n",
    "    def __init__(self, strength=0.0):\n",
    "        self.strength = float(strength) # Ensure strength is a float\n",
    "\n",
    "    def __call__(self, w):\n",
    "        # The regularization function\n",
    "        return self.strength * tf.reduce_sum(tf.pow(tf.abs(w), 3))\n",
    "\n",
    "    def get_config(self):\n",
    "        # Required for Keras to save/load the model\n",
    "        return {'strength': self.strength}\n",
    "# -----------------------------------\n",
    "\n",
    "print(f\"\\nüß† NEW: Starting Regularization Experiment on best model ({best_combo['optimizer']} + {best_combo['activation']})...\")\n",
    "\n",
    "# Define regularization strengths\n",
    "reg_strength = 0.001 # You can tune this\n",
    "regularizers_to_test = {\n",
    "    \"Original (No Regularization)\": None,\n",
    "    \"L1 Regularization\": l1(reg_strength),\n",
    "    \"L2 Regularization\": l2(reg_strength),\n",
    "    \"L1+L2 Regularization\": l1_l2(reg_strength, reg_strength),\n",
    "    \"Custom L3 Regularization\": L3Regularizer(strength=reg_strength) # Add custom L3\n",
    "}\n",
    "\n",
    "reg_results_list = []\n",
    "\n",
    "for reg_name, reg_func in regularizers_to_test.items():\n",
    "    print(f\"\\nüöÄ Training model with {reg_name}...\")\n",
    "\n",
    "    # Create the model using the best combo and the specified regularizer\n",
    "    model = create_cnn_model(\n",
    "        optimizer_name=best_combo['optimizer'],\n",
    "        activation_name=best_combo['activation'],\n",
    "        regularizer=reg_func\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_cnn, y_train_cnn,\n",
    "              epochs=epochs_to_run,\n",
    "              batch_size=32,\n",
    "              verbose=0,\n",
    "              validation_split=0.1)\n",
    "\n",
    "    # Evaluate\n",
    "    loss, accuracy = model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
    "\n",
    "    print(f\"‚úÖ {reg_name} - Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Store result for table\n",
    "    reg_results_list.append({\n",
    "        \"Model\": reg_name,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "    # Add to main accuracy dictionary (except for original)\n",
    "    if reg_name != \"Original (No Regularization)\":\n",
    "        model_accuracies[f\"CNN (Best + {reg_name})\"] = accuracy\n",
    "\n",
    "print(\"\\n\\n‚úÖ CNN Regularization Experiment Complete!\")\n",
    "\n",
    "# --- Generate and Print Regularization Table ---\n",
    "reg_results_df = pd.DataFrame(reg_results_list)\n",
    "reg_results_df[\"Accuracy\"] = reg_results_df[\"Accuracy\"].map(lambda x: f\"{x:.4f}\") # Format\n",
    "\n",
    "print(\"\\nüìä CNN Regularization Accuracy Comparison (Original vs L1 vs L2 vs L3)\")\n",
    "print(tabulate(reg_results_df, headers='keys', tablefmt='pretty', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utX_Ato4J3I5",
    "outputId": "ba9974d3-b27c-48b7-e79e-885feb916fca"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüß© Building CNN + ML Hybrid Models...\")\n",
    "\n",
    "# We use the best CNN model from our experiment as the feature extractor\n",
    "print(f\"Using {best_combo['optimizer']}+{best_combo['activation']} for hybrid base.\")\n",
    "cnn_model_for_hybrid = create_cnn_model(\n",
    "    optimizer_name=best_combo['optimizer'],\n",
    "    activation_name=best_combo['activation']\n",
    ")\n",
    "cnn_model_for_hybrid.fit(X_train_cnn, y_train_cnn, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Extract features from CNN (remove final classification layer)\n",
    "feature_extractor = Sequential(cnn_model_for_hybrid.layers[:-1])\n",
    "cnn_features_train = feature_extractor.predict(X_train_cnn)\n",
    "cnn_features_test = feature_extractor.predict(X_test_cnn)\n",
    "print(\"‚úÖ CNN features extracted.\")\n",
    "\n",
    "# Define ML models for hybrid use\n",
    "hybrid_models = {\n",
    "    \"CNN + RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"CNN + XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"CNN + AdaBoost\": AdaBoostClassifier(n_estimators=150, random_state=42),\n",
    "    \"CNN + CatBoost\": CatBoostClassifier(verbose=0, allow_writing_files=False, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate hybrid models\n",
    "for name, model in hybrid_models.items():\n",
    "    print(f\"üöÄ Training {name} ...\")\n",
    "    model.fit(cnn_features_train, y_train)\n",
    "    y_pred = model.predict(cnn_features_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    model_accuracies[name] = acc\n",
    "    print(f\"‚úÖ {name} Accuracy: {acc:.4f}\")\n",
    "\n",
    "# --- Bagging Combinations ---\n",
    "print(\"\\nüß© Building Bagging Model Combinations...\")\n",
    "bagging_combos = {\n",
    "    \"Bagging_RF_on_XGB\": BaggingClassifier(estimator=xgb_clf, n_estimators=10, random_state=42),\n",
    "    \"Bagging_RF_on_CatBoost\": BaggingClassifier(estimator=cat_clf, n_estimators=10, random_state=42),\n",
    "    \"Bagging_XGB_on_RF\": BaggingClassifier(estimator=rf_clf, n_estimators=10, random_state=42),\n",
    "}\n",
    "\n",
    "for name, bag_model in bagging_combos.items():\n",
    "    print(f\"\\nüöÄ Training {name} ...\")\n",
    "    bag_model.fit(X_train, y_train)\n",
    "    y_pred = bag_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    model_accuracies[name] = acc\n",
    "    print(f\"‚úÖ {name} Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Hybrid and Bagging models complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "id": "-omU8pXmJ6LA",
    "outputId": "d598078f-659f-4fa4-c63f-1a5232edff2e"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüìä Generating Final Combined Model Accuracy Chart...\")\n",
    "\n",
    "# Sort the models by accuracy for better visualization\n",
    "sorted_accuracies = sorted(model_accuracies.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_model_names = [item[0] for item in sorted_accuracies]\n",
    "sorted_model_scores = [item[1] for item in sorted_accuracies]\n",
    "\n",
    "plt.figure(figsize=(24, 12)) # Increased size for all models\n",
    "bars = plt.bar(sorted_model_names, sorted_model_scores, color=plt.cm.Paired(np.arange(len(sorted_model_names))))\n",
    "\n",
    "plt.title(\"üåæ Final Combined Model Accuracy Comparison (All Models)\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy Score\", fontsize=14)\n",
    "plt.xlabel(\"Model\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10) # Rotate labels\n",
    "plt.yticks(fontsize=10)\n",
    "plt.ylim(0, 1.05) # Set y-axis limit\n",
    "plt.tight_layout() # Adjust layout\n",
    "\n",
    "# Add text labels on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.005, f'{yval:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Save the final chart\n",
    "plt.savefig(\"final_combined_accuracy_comparison.png\", dpi=300)\n",
    "# plt.show() # In Colab, the plot will display automatically\n",
    "\n",
    "print(\"\\n‚úÖ Final Combined Accuracy Chart Saved: final_combined_accuracy_comparison.png\")\n",
    "print(\"\\nüéâ All tasks complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EfkkmkEjXuLZ",
    "outputId": "2fcec58e-3692-43e1-f5cb-e5be8ef2f7fa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset (Make sure to point to your original file)\n",
    "print(\"üîπ Loading raw dataset to check imbalance...\")\n",
    "try:\n",
    "    # Use the filename from section-3\n",
    "    df_raw = pd.read_csv(\"Crop Recommendation using Soil Properties and Weather Prediction.csv\")\n",
    "    print(f\"Dataset shape: {df_raw.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: Original dataset file not found. Please ensure 'Crop Recommendation using Soil Properties and Weather Prediction.csv' is uploaded.\")\n",
    "    raise\n",
    "\n",
    "# Check class distribution of the target variable ('label')\n",
    "target_col = 'label'\n",
    "class_counts = df_raw[target_col].value_counts()\n",
    "class_proportions = df_raw[target_col].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nüìä Class Distribution (Original Dataset):\")\n",
    "print(class_counts)\n",
    "print(\"\\nPercentage Distribution:\")\n",
    "print(class_proportions.map(lambda x: f\"{x:.2f}%\"))\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(y=df_raw[target_col], order=class_counts.index, palette=\"viridis\")\n",
    "plt.title(f'Class Distribution of Target Variable: {target_col}')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Crop Label')\n",
    "plt.show()\n",
    "\n",
    "# Conclusion based on proportions\n",
    "min_class_prop = class_proportions.min()\n",
    "max_class_prop = class_proportions.max()\n",
    "imbalance_ratio = max_class_prop / min_class_prop\n",
    "\n",
    "print(f\"\\n‚úÖ Imbalance Check Complete!\")\n",
    "if imbalance_ratio > 2: # Simple heuristic: largest class is more than 2x the smallest\n",
    "    print(f\"‚ö†Ô∏è **Result: The dataset is IMBLANCED!** (Largest class is {imbalance_ratio:.2f}x the size of the smallest.)\")\n",
    "else:\n",
    "    print(\"üëç **Result: The dataset appears to be relatively balanced.**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3b4kqJKXvCc",
    "outputId": "638dc4d3-e28d-4665-9728-f3caf59cd37e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# --- Preprocessing without SMOTE ---\n",
    "print(\"üîπ Loading raw dataset for UNBALANCED training...\")\n",
    "try:\n",
    "    df_unbalanced = pd.read_csv(\"Crop Recommendation using Soil Properties and Weather Prediction.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: Original dataset file not found.\")\n",
    "    raise\n",
    "\n",
    "# Identify categorical (non-numeric) columns and encode\n",
    "target_col = 'label'\n",
    "categorical_cols = df_unbalanced.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols.remove(target_col)\n",
    "\n",
    "le_target = LabelEncoder()\n",
    "df_unbalanced['label_encoded'] = le_target.fit_transform(df_unbalanced[target_col])\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_unbalanced[col] = le.fit_transform(df_unbalanced[col])\n",
    "\n",
    "# Split features and target\n",
    "X_unbalanced = df_unbalanced.drop([target_col, 'label_encoded'], axis=1)\n",
    "y_unbalanced = df_unbalanced['label_encoded']\n",
    "\n",
    "# Scale numeric features (using the same logic as before)\n",
    "scaler_unbalanced = StandardScaler()\n",
    "X_scaled_unbalanced = scaler_unbalanced.fit_transform(X_unbalanced)\n",
    "X_scaled_unbalanced = pd.DataFrame(X_scaled_unbalanced, columns=X_unbalanced.columns)\n",
    "\n",
    "print(f\"Features (X_unbalanced) shape: {X_scaled_unbalanced.shape}\")\n",
    "\n",
    "# Split into Train/Test sets (Stratify is critical for imbalanced data)\n",
    "X_train_unbalanced, X_test_unbalanced, y_train_unbalanced, y_test_unbalanced = train_test_split(\n",
    "    X_scaled_unbalanced, y_unbalanced, test_size=0.2, random_state=42, stratify=y_unbalanced\n",
    ")\n",
    "print(\"‚úÖ Data split complete ‚Äî UNBALANCED data\")\n",
    "print(f\"Train samples: {X_train_unbalanced.shape[0]}, Test samples: {X_test_unbalanced.shape[0]}\")\n",
    "\n",
    "# --- Training Models on Unbalanced Data ---\n",
    "print(\"\\nüöÄ Training standard ML models on UNBALANCED data...\")\n",
    "\n",
    "# Re-define Models (same as before)\n",
    "rf_clf_unb = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "xgb_clf_unb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "ada_clf_unb = AdaBoostClassifier(n_estimators=150, random_state=42)\n",
    "cat_clf_unb = CatBoostClassifier(verbose=0, allow_writing_files=False, random_state=42)\n",
    "\n",
    "# Dictionary to hold unbalanced model accuracies\n",
    "unbalanced_accuracies = {}\n",
    "\n",
    "# Train Models\n",
    "print(\"... Random Forest\")\n",
    "rf_clf_unb.fit(X_train_unbalanced, y_train_unbalanced)\n",
    "unbalanced_accuracies[\"Random Forest (Unbalanced)\"] = accuracy_score(y_test_unbalanced, rf_clf_unb.predict(X_test_unbalanced))\n",
    "\n",
    "print(\"... XGBoost\")\n",
    "xgb_clf_unb.fit(X_train_unbalanced, y_train_unbalanced)\n",
    "unbalanced_accuracies[\"XGBoost (Unbalanced)\"] = accuracy_score(y_test_unbalanced, xgb_clf_unb.predict(X_test_unbalanced))\n",
    "\n",
    "print(\"... AdaBoost\")\n",
    "ada_clf_unb.fit(X_train_unbalanced, y_train_unbalanced)\n",
    "unbalanced_accuracies[\"AdaBoost (Unbalanced)\"] = accuracy_score(y_test_unbalanced, ada_clf_unb.predict(X_test_unbalanced))\n",
    "\n",
    "print(\"... CatBoost\")\n",
    "cat_clf_unb.fit(X_train_unbalanced, y_train_unbalanced)\n",
    "unbalanced_accuracies[\"CatBoost (Unbalanced)\"] = accuracy_score(y_test_unbalanced, cat_clf_unb.predict(X_test_unbalanced))\n",
    "\n",
    "print(\"\\n‚úÖ All unbalanced base models trained successfully!\")\n",
    "for name, acc in unbalanced_accuracies.items():\n",
    "    print(f\"üî∏ {name} Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Store the final accuracies with SMOTE results (assuming model_accuracies is from your previous code)\n",
    "global final_model_accuracies\n",
    "final_model_accuracies = model_accuracies.copy() # Make a copy of the existing (SMOTE/CNN) results\n",
    "final_model_accuracies.update(unbalanced_accuracies) # Add the new unbalanced results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VnmfxDbSXzLU",
    "outputId": "1a29e3d8-583a-47c4-ca5f-f67a627b3f80"
   },
   "outputs": [],
   "source": [
    "# Assuming 'final_model_accuracies' contains all results from previous cells\n",
    "# and 'model_accuracies' from the original cells (SMOTE, Ensemble, CNN)\n",
    "\n",
    "print(\"\\nüèÜ Final Model Comparison: Balanced (SMOTE) vs. Unbalanced Results\")\n",
    "\n",
    "# Filter for the core four ML models for direct comparison\n",
    "comparison_data = []\n",
    "\n",
    "# Get the results for the models trained on the original SMOTE-balanced data\n",
    "smote_results = {\n",
    "    \"Random Forest (Balanced)\": final_model_accuracies.get(\"Random Forest\"),\n",
    "    \"XGBoost (Balanced)\": final_model_accuracies.get(\"XGBoost\"),\n",
    "    \"AdaBoost (Balanced)\": final_model_accuracies.get(\"AdaBoost\"),\n",
    "    \"CatBoost (Balanced)\": final_model_accuracies.get(\"CatBoost\"),\n",
    "}\n",
    "\n",
    "# Get the results for the models trained on the new Unbalanced data\n",
    "unbalanced_results = {\n",
    "    \"Random Forest (Unbalanced)\": final_model_accuracies.get(\"Random Forest (Unbalanced)\"),\n",
    "    \"XGBoost (Unbalanced)\": final_model_accuracies.get(\"XGBoost (Unbalanced)\"),\n",
    "    \"AdaBoost (Unbalanced)\": final_model_accuracies.get(\"AdaBoost (Unbalanced)\"),\n",
    "    \"CatBoost (Unbalanced)\": final_model_accuracies.get(\"CatBoost (Unbalanced)\"),\n",
    "}\n",
    "\n",
    "# Combine into a comparison list\n",
    "base_models = [\"Random Forest\", \"XGBoost\", \"AdaBoost\", \"CatBoost\"]\n",
    "\n",
    "for model_name in base_models:\n",
    "    balanced_key = f\"{model_name} (Balanced)\"\n",
    "    unbalanced_key = f\"{model_name} (Unbalanced)\"\n",
    "\n",
    "    acc_balanced = smote_results.get(balanced_key, 0)\n",
    "    acc_unbalanced = unbalanced_results.get(unbalanced_key, 0)\n",
    "\n",
    "    # Calculate difference\n",
    "    difference = acc_balanced - acc_unbalanced\n",
    "\n",
    "    comparison_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy (SMOTE Balanced)\": f\"{acc_balanced:.4f}\",\n",
    "        \"Accuracy (Unbalanced)\": f\"{acc_unbalanced:.4f}\",\n",
    "        \"Difference (B - UB)\": f\"{difference:+.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nüìà Base ML Model Accuracy Comparison\")\n",
    "print(tabulate(comparison_df, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "\n",
    "print(\"\\n---\")\n",
    "\n",
    "# Find the Overall Best Model (Including Ensembles, CNN, Hybrids, and Unbalanced)\n",
    "best_model_name = max(final_model_accuracies, key=final_model_accuracies.get)\n",
    "best_accuracy = final_model_accuracies[best_model_name]\n",
    "\n",
    "# Create a full results table\n",
    "full_results_list = []\n",
    "for name, acc in sorted(final_model_accuracies.items(), key=lambda item: item[1], reverse=True):\n",
    "    full_results_list.append({\"Model Name\": name, \"Accuracy\": f\"{acc:.4f}\"})\n",
    "\n",
    "print(\"\\nü•á **OVERALL PERFORMANCE LEADERBOARD**\")\n",
    "print(f\"The highest accuracy was achieved by: **{best_model_name}** with **{best_accuracy:.4f}**\")\n",
    "print(\"\\nAll Model Accuracies:\")\n",
    "print(tabulate(full_results_list, headers='keys', tablefmt='fancy_grid', showindex=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
